{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "# def read_files(folder_path, mask_path):\n",
    "\n",
    "#     tiff_files = [file for file in os.listdir(folder_path) if file.endswith('.tiff')]\n",
    "\n",
    "#     images = []\n",
    "#     masks = []\n",
    "#     for file in tiff_files:\n",
    "#         image = Image.open(os.path.join(folder_path, file))\n",
    "#         images.append(np.array(image))\n",
    "#         mask = Image.open(os.path.join(mask_path, os.path.splitext(file)[0] + '_mask.png'))\n",
    "#         masks.append(np.array(mask))\n",
    "\n",
    "#     return np.array(images), np.array(masks)\n",
    "\n",
    "def read_h5_file(file_path=\"/home/grad/ccomp/21/nuneslima/Seismic-Analysis/penobscot/dataset.h5\"):\n",
    "    f = h5py.File(file_path,'r')\n",
    "    images=f['features']\n",
    "    labels=f['label']\n",
    "    return np.squeeze(np.array(images)), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "def divide_into_patches(images, patch_h, patch_w, stride_h, stride_w):\n",
    "    num_images, height, width= images.shape\n",
    "    \n",
    "    patches_per_dim_h = (height - patch_h) // stride_h + 1\n",
    "    patches_per_dim_w = (width - patch_w) // stride_w + 1\n",
    "    \n",
    "    num_patches_per_image = patches_per_dim_h * patches_per_dim_w\n",
    "    \n",
    "    patches = np.zeros((num_images * num_patches_per_image, patch_h, patch_w), dtype=images.dtype)\n",
    "    \n",
    "    idx = 0\n",
    "    for image in images:\n",
    "        for h in range(0, height - patch_h + 1, stride_h):\n",
    "            for w in range(0, width - patch_w + 1, stride_w):\n",
    "                patch = image[h:h+patch_h, w:w+patch_w]\n",
    "                patches[idx] = patch\n",
    "                idx += 1\n",
    "                \n",
    "    return patches\n",
    "\n",
    "\n",
    "def data_split(data, train_ratio=0.7, test_ratio=0.2, val_ratio=0.1):\n",
    "    num_samples = data.shape[0]\n",
    "    \n",
    "    train_size = int(train_ratio * num_samples)\n",
    "    val_size = int(val_ratio * num_samples)\n",
    "    \n",
    "    train_indices = np.arange(0, train_size)\n",
    "    val_indices = np.arange(train_size, train_size+val_size)\n",
    "    test_indices = np.arange(train_size+val_size, num_samples)\n",
    "    \n",
    "    train_set = data[train_indices]\n",
    "    test_set = data[test_indices]\n",
    "    val_set = data[val_indices]\n",
    "    \n",
    "    return train_set, test_set, val_set\n",
    "\n",
    "\n",
    "def majority_class(images, masks, threshold_percentage=0.7):\n",
    "    n, a, b = masks.shape\n",
    "    majority_classes = []\n",
    "    majority_images=[]\n",
    "\n",
    "    for i in range(n):\n",
    "        sample = masks[i]\n",
    "        flattened_sample = sample.flatten()\n",
    "        unique_classes, counts = np.unique(flattened_sample, return_counts=True)\n",
    "        max_count = np.max(counts)\n",
    "        total_count = np.sum(counts)\n",
    "        if max_count / total_count >= threshold_percentage:\n",
    "            majority_class_index = np.argmax(counts)\n",
    "            majority_class = unique_classes[majority_class_index]\n",
    "            majority_classes.append(majority_class)\n",
    "            majority_images.append(images[i])\n",
    "\n",
    "\n",
    "    return np.array(majority_images), np.array(majority_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def penobscot_data_seg(patch_h, patch_w,stride_h, stride_w,train_ratio=0.7, test_ratio=0.2, val_ratio=0.1):\n",
    "    #inlines, masks_in=read_files(\"/home/grad/ccomp/21/nuneslima/Datasets/Penobscot/inlines\",'/home/grad/ccomp/21/nuneslima/Datasets/Penobscot/masks')\n",
    "    images, masks=read_h5_file()\n",
    "    images = ((images + 32767) / 65534) * 255\n",
    "    images=images.astype(np.uint8)\n",
    "    patch=divide_into_patches(images,patch_h, patch_w,stride_h, stride_w)\n",
    "    patch_mask=divide_into_patches(masks,patch_h, patch_w,stride_h, stride_w)\n",
    "\n",
    "    train,test,val=data_split(patch, train_ratio=train_ratio, test_ratio=test_ratio, val_ratio=val_ratio)\n",
    "    mask_train,mask_test,mask_val=data_split(patch_mask, train_ratio=train_ratio, test_ratio=test_ratio, val_ratio=val_ratio)\n",
    "\n",
    "\n",
    "    return train, mask_train, test, mask_test, val, mask_val\n",
    "\n",
    "\n",
    "def penobscot_data(patch,stride):\n",
    "    inlines, masks_in=read_h5_file(\"/home/grad/ccomp/21/nuneslima/Datasets/Penobscot/inlines\",'/home/grad/ccomp/21/nuneslima/Datasets/Penobscot/masks')\n",
    "    inlines = ((inlines + 32767) / 65534) * 255\n",
    "    inlines=inlines.astype(np.uint8)\n",
    "    #crosslines, masks_cross=read_files(\"/home/grad/ccomp/21/nuneslima/Datasets/Penobscot/crosslines\",'/home/grad/ccomp/21/nuneslima/Datasets/Penobscot/masks')\n",
    "    patch_inline=divide_into_patches(inlines,patch,stride)\n",
    "    #patch_crossline=divide_into_patches(crosslines,patch,stride)\n",
    "    patch_mask_in=divide_into_patches(masks_in,patch,stride)\n",
    "    #patch_mask_cross=divide_into_patches(masks_cross,patch,stride)\n",
    "\n",
    "    in_train,in_test,in_val=data_split(patch_inline)\n",
    "    mask_in_train,mask_in_test,mask_in_val=data_split(patch_mask_in)\n",
    "    # cross_train,cross_test,cross_val=data_split(patch_crossline)\n",
    "    # mask_cross_train,mask_cross_test,mask_cross_val=data_split(patch_mask_cross)\n",
    "\n",
    "    # trainX=np.append(in_train,cross_train, axis=0)\n",
    "    # trainY=np.append(mask_in_train,mask_cross_train, axis=0)\n",
    "    # testX=np.append(in_test,cross_test, axis=0)\n",
    "    # testY=np.append(mask_in_test,mask_cross_test, axis=0)\n",
    "    # valX=np.append(in_val,cross_val, axis=0)\n",
    "    # valY=np.append(mask_in_val,mask_cross_val, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    patch_train,train_labels = majority_class(in_train, mask_in_train, 0.7)\n",
    "    patch_test,test_labels = majority_class(in_test, mask_in_test, 0.7)\n",
    "    patch_val,val_labels = majority_class(in_val, mask_in_val, 0.7)\n",
    "\n",
    "\n",
    "    return patch_train, train_labels, patch_test, test_labels, patch_val, val_labels\n",
    "\n",
    "    #return trainX, majority_class(trainY), testX, majority_class(testY), valX, majority_class(valY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(601, 1501, 481)\n",
      "(601, 1501, 481)\n"
     ]
    }
   ],
   "source": [
    "images, labels=read_h5_file()\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(601, 1501, 481)\n",
      "(601, 1501, 481)\n"
     ]
    }
   ],
   "source": [
    "images = ((images + 32767) / 65534) * 255\n",
    "images=images.astype(np.uint8)\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "reshaped_arr = labels.reshape(-1, labels.shape[-1])\n",
    "\n",
    "# Get the unique values\n",
    "unique_values = np.unique(reshaped_arr)\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129816, 200, 100)\n",
      "(129816, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "patch_h=200\n",
    "patch_w=100\n",
    "stride_h=50\n",
    "stride_w=50\n",
    "patch=divide_into_patches(images,patch_h, patch_w,stride_h, stride_w)\n",
    "patch_mask=divide_into_patches(labels,patch_h, patch_w,stride_h, stride_w)\n",
    "print(patch.shape)\n",
    "print(patch_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90871, 200, 100) (90871, 200, 100) (25964, 200, 100) (25964, 200, 100) (12981, 200, 100) (12981, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "train_ratio=0.7\n",
    "test_ratio=0,2\n",
    "val_ratio=0.1\n",
    "train,test,val=data_split(patch, train_ratio=train_ratio, test_ratio=test_ratio, val_ratio=val_ratio)\n",
    "mask_train,mask_test,mask_val=data_split(patch_mask, train_ratio=train_ratio, test_ratio=test_ratio, val_ratio=val_ratio)\n",
    "print(train.shape, mask_train.shape, test.shape, mask_test.shape, val.shape, mask_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rockml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
